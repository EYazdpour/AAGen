{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DeepFool.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "LLUZb-SYR86C"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "def deepfool(image, model, num_classes=10, overshoot=0.02, max_iter=50, shape=(28, 28, 1)):\n",
        "\n",
        "    image_array = np.array(image)\n",
        "    image_norm = tf.cast(image_array / 255.0 - 0.5, tf.float32)\n",
        "    image_norm = np.reshape(image_norm, shape)  \n",
        "    image_norm = image_norm[tf.newaxis, ...]  \n",
        "\n",
        "    f_image = model(image_norm).numpy().flatten()\n",
        "    I = (np.array(f_image)).flatten().argsort()[::-1]\n",
        "    I = I[0:num_classes]\n",
        "    label = I[0]\n",
        "\n",
        "    input_shape = np.shape(image_norm)\n",
        "    pert_image = copy.deepcopy(image_norm)\n",
        "    w = np.zeros(input_shape)\n",
        "    r_tot = np.zeros(input_shape)\n",
        "\n",
        "    loop_i = 0\n",
        "    x = tf.Variable(pert_image)\n",
        "    fs = model(x)\n",
        "    k_i = label\n",
        "\n",
        "    print(fs) \n",
        "\n",
        "    def loss_func(logits, I, k):\n",
        "        return logits[0, I[k]]\n",
        "\n",
        "    while k_i == label and loop_i < max_iter:\n",
        "\n",
        "        pert = np.inf\n",
        "\n",
        "        one_hot_label_0 = tf.one_hot(label, num_classes)\n",
        "        with tf.GradientTape() as tape:\n",
        "            tape.watch(x)\n",
        "            fs = model(x)\n",
        "            loss_value = loss_func(fs, I, 0)\n",
        "        grad_orig = tape.gradient(loss_value, x)\n",
        "\n",
        "        for k in range(1, num_classes):\n",
        "            one_hot_label_k = tf.one_hot(I[k], num_classes)\n",
        "            with tf.GradientTape() as tape:\n",
        "                tape.watch(x)\n",
        "                fs = model(x)\n",
        "                loss_value = loss_func(fs, I, k)\n",
        "            cur_grad = tape.gradient(loss_value, x)\n",
        "\n",
        "            w_k = cur_grad - grad_orig\n",
        "\n",
        "            f_k = (fs[0, I[k]] - fs[0, I[0]]).numpy()\n",
        "\n",
        "            pert_k = abs(f_k) / np.linalg.norm(tf.reshape(w_k, [-1]))\n",
        "\n",
        "            if pert_k < pert:\n",
        "                pert = pert_k\n",
        "                w = w_k\n",
        "\n",
        "        r_i = (pert + 1e-4) * w / np.linalg.norm(w)\n",
        "        r_tot = np.float32(r_tot + r_i)\n",
        "\n",
        "        pert_image = image_norm + (1 + overshoot) * r_tot\n",
        "\n",
        "        x = tf.Variable(pert_image)\n",
        "\n",
        "        fs = model(x)\n",
        "        k_i = np.argmax(np.array(fs).flatten())\n",
        "\n",
        "        loop_i += 1\n",
        "\n",
        "    r_tot = (1 + overshoot) * r_tot\n",
        "\n",
        "    return r_tot, loop_i, label, k_i, pert_image"
      ],
      "execution_count": 2,
      "outputs": []
    }
  ]
}